{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "23edf72c",
   "metadata": {},
   "source": [
    "### 1) Label Encoding Technique "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "836b1078",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.datasets import load_iris\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ef9dd2dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "iris=load_iris()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fb0adf8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "x=pd.DataFrame(iris.data,columns=iris.feature_names)\n",
    "y=iris.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "151a26f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal length (cm)</th>\n",
       "      <th>sepal width (cm)</th>\n",
       "      <th>petal length (cm)</th>\n",
       "      <th>petal width (cm)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)\n",
       "0                5.1               3.5                1.4               0.2\n",
       "1                4.9               3.0                1.4               0.2\n",
       "2                4.7               3.2                1.3               0.2\n",
       "3                4.6               3.1                1.5               0.2\n",
       "4                5.0               3.6                1.4               0.2"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ea08db19",
   "metadata": {},
   "outputs": [],
   "source": [
    "#make it categorical\n",
    "y=pd.Categorical.from_codes(y,iris.target_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "973fb745",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['setosa', 'setosa', 'setosa', 'setosa', 'setosa', ..., 'virginica', 'virginica', 'virginica', 'virginica', 'virginica']\n",
       "Length: 150\n",
       "Categories (3, object): ['setosa', 'versicolor', 'virginica']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "57be9284",
   "metadata": {},
   "outputs": [],
   "source": [
    "le=LabelEncoder()\n",
    "y=le.fit_transform(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "092b65f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d858e91e",
   "metadata": {},
   "source": [
    "### 2) One hot encoding technique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "d3cb413a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "7d4da531",
   "metadata": {},
   "outputs": [],
   "source": [
    "iris=load_iris()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "57c81bec",
   "metadata": {},
   "outputs": [],
   "source": [
    "x=pd.DataFrame(iris.data, columns=iris.feature_names)\n",
    "y=iris.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "054a6afd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "3ad9f20e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['setosa', 'setosa', 'setosa', 'setosa', 'setosa', ..., 'virginica', 'virginica', 'virginica', 'virginica', 'virginica']\n",
       "Length: 150\n",
       "Categories (3, object): ['setosa', 'versicolor', 'virginica']"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y=pd.Categorical.from_codes(y,iris.target_names)\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "fecbf80b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ohe=OneHotEncoder()\n",
    "y_encoded=ohe.fit_transform(y.reshape(-1,1))\n",
    "y_encoded=y_encoded.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "d8d059d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.]])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_encoded"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d40385e",
   "metadata": {},
   "source": [
    "### 3) Word embedding technique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "b45e9f93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gensim in c:\\users\\hp\\anaconda3\\envs\\aml-manyahegde\\lib\\site-packages (4.3.1)\n",
      "Requirement already satisfied: scipy>=1.7.0 in c:\\users\\hp\\anaconda3\\envs\\aml-manyahegde\\lib\\site-packages (from gensim) (1.10.1)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in c:\\users\\hp\\anaconda3\\envs\\aml-manyahegde\\lib\\site-packages (from gensim) (6.3.0)\n",
      "Requirement already satisfied: numpy>=1.18.5 in c:\\users\\hp\\anaconda3\\envs\\aml-manyahegde\\lib\\site-packages (from gensim) (1.24.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "9b4518ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nltk\n",
      "  Downloading nltk-3.8.1-py3-none-any.whl (1.5 MB)\n",
      "     ---------------------------------------- 0.0/1.5 MB ? eta -:--:--\n",
      "     ---------------------------------------- 0.0/1.5 MB ? eta -:--:--\n",
      "     - -------------------------------------- 0.0/1.5 MB 653.6 kB/s eta 0:00:03\n",
      "     - -------------------------------------- 0.1/1.5 MB 653.6 kB/s eta 0:00:03\n",
      "     - -------------------------------------- 0.1/1.5 MB 653.6 kB/s eta 0:00:03\n",
      "     ---- ----------------------------------- 0.2/1.5 MB 748.1 kB/s eta 0:00:02\n",
      "     ----- ---------------------------------- 0.2/1.5 MB 655.9 kB/s eta 0:00:03\n",
      "     ----- ---------------------------------- 0.2/1.5 MB 655.1 kB/s eta 0:00:02\n",
      "     ------- -------------------------------- 0.3/1.5 MB 737.3 kB/s eta 0:00:02\n",
      "     ------- -------------------------------- 0.3/1.5 MB 737.3 kB/s eta 0:00:02\n",
      "     ------- -------------------------------- 0.3/1.5 MB 737.3 kB/s eta 0:00:02\n",
      "     ------- -------------------------------- 0.3/1.5 MB 737.3 kB/s eta 0:00:02\n",
      "     ------- -------------------------------- 0.3/1.5 MB 737.3 kB/s eta 0:00:02\n",
      "     ------- -------------------------------- 0.3/1.5 MB 737.3 kB/s eta 0:00:02\n",
      "     -------- ------------------------------- 0.3/1.5 MB 504.4 kB/s eta 0:00:03\n",
      "     -------- ------------------------------- 0.3/1.5 MB 466.1 kB/s eta 0:00:03\n",
      "     ----------- ---------------------------- 0.4/1.5 MB 573.4 kB/s eta 0:00:02\n",
      "     ----------- ---------------------------- 0.4/1.5 MB 573.4 kB/s eta 0:00:02\n",
      "     ------------ --------------------------- 0.5/1.5 MB 558.1 kB/s eta 0:00:02\n",
      "     ------------- -------------------------- 0.5/1.5 MB 561.7 kB/s eta 0:00:02\n",
      "     ------------- -------------------------- 0.5/1.5 MB 546.4 kB/s eta 0:00:02\n",
      "     ---------------- ----------------------- 0.6/1.5 MB 623.5 kB/s eta 0:00:02\n",
      "     ----------------- ---------------------- 0.7/1.5 MB 635.7 kB/s eta 0:00:02\n",
      "     ------------------- -------------------- 0.7/1.5 MB 673.9 kB/s eta 0:00:02\n",
      "     --------------------- ------------------ 0.8/1.5 MB 719.1 kB/s eta 0:00:01\n",
      "     ----------------------- ---------------- 0.9/1.5 MB 750.5 kB/s eta 0:00:01\n",
      "     ----------------------- ---------------- 0.9/1.5 MB 750.5 kB/s eta 0:00:01\n",
      "     ----------------------- ---------------- 0.9/1.5 MB 750.5 kB/s eta 0:00:01\n",
      "     ------------------------ --------------- 0.9/1.5 MB 686.8 kB/s eta 0:00:01\n",
      "     --------------------------- ------------ 1.0/1.5 MB 769.5 kB/s eta 0:00:01\n",
      "     --------------------------- ------------ 1.0/1.5 MB 769.5 kB/s eta 0:00:01\n",
      "     ----------------------------- ---------- 1.1/1.5 MB 755.4 kB/s eta 0:00:01\n",
      "     ------------------------------ --------- 1.2/1.5 MB 764.6 kB/s eta 0:00:01\n",
      "     -------------------------------- ------- 1.2/1.5 MB 794.3 kB/s eta 0:00:01\n",
      "     ----------------------------------- ---- 1.3/1.5 MB 828.7 kB/s eta 0:00:01\n",
      "     ----------------------------------- ---- 1.3/1.5 MB 828.7 kB/s eta 0:00:01\n",
      "     ----------------------------------- ---- 1.3/1.5 MB 828.7 kB/s eta 0:00:01\n",
      "     ----------------------------------- ---- 1.3/1.5 MB 828.7 kB/s eta 0:00:01\n",
      "     ----------------------------------- ---- 1.3/1.5 MB 828.7 kB/s eta 0:00:01\n",
      "     ----------------------------------- ---- 1.3/1.5 MB 828.7 kB/s eta 0:00:01\n",
      "     ----------------------------------- ---- 1.3/1.5 MB 828.7 kB/s eta 0:00:01\n",
      "     ----------------------------------- ---- 1.3/1.5 MB 828.7 kB/s eta 0:00:01\n",
      "     ---------------------------------------- 1.5/1.5 MB 762.0 kB/s eta 0:00:00\n",
      "Collecting tqdm\n",
      "  Downloading tqdm-4.65.0-py3-none-any.whl (77 kB)\n",
      "     ---------------------------------------- 0.0/77.1 kB ? eta -:--:--\n",
      "     ----- ---------------------------------- 10.2/77.1 kB ? eta -:--:--\n",
      "     -------------------- ----------------- 41.0/77.1 kB 653.6 kB/s eta 0:00:01\n",
      "     -------------------------------------- 77.1/77.1 kB 607.8 kB/s eta 0:00:00\n",
      "Collecting regex>=2021.8.3\n",
      "  Downloading regex-2023.5.5-cp39-cp39-win_amd64.whl (267 kB)\n",
      "     ---------------------------------------- 0.0/268.0 kB ? eta -:--:--\n",
      "     ---------- ---------------------------- 71.7/268.0 kB 4.1 MB/s eta 0:00:01\n",
      "     --------------- ---------------------- 112.6/268.0 kB 3.3 MB/s eta 0:00:01\n",
      "     ------------------------------------ - 256.0/268.0 kB 2.0 MB/s eta 0:00:01\n",
      "     -------------------------------------- 268.0/268.0 kB 2.1 MB/s eta 0:00:00\n",
      "Requirement already satisfied: joblib in c:\\users\\hp\\anaconda3\\envs\\aml-manyahegde\\lib\\site-packages (from nltk) (1.2.0)\n",
      "Collecting click\n",
      "  Using cached click-8.1.3-py3-none-any.whl (96 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\hp\\anaconda3\\envs\\aml-manyahegde\\lib\\site-packages (from click->nltk) (0.4.6)\n",
      "Installing collected packages: tqdm, regex, click, nltk\n",
      "Successfully installed click-8.1.3 nltk-3.8.1 regex-2023.5.5 tqdm-4.65.0\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "c0548b8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "9c191c58",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sample text to be preprocessed and embedded\n",
    "text='This a sample text for preprocessing and word embedding using Python'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "d8dfa183",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenize the text into individual words\n",
    "words=nltk.word_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "f419352b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "ddb74c7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#preprocess the words by converting to lowercase and removing punctuation\n",
    "processed_words=[word.lower() for word in words if word.isalpha() and word.lower() not in nltk.corpus.stopwords.words()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "2ded56dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate word embeddings using the Word2Vec algorithm from gensim\n",
    "model=Word2Vec([processed_words],vector_size=100, window=5, min_count=1, workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "31685a5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the embedding vector for a specific word\n",
    "word='python'\n",
    "embedding_vector=model.wv[word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "6bfb46d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-5.3622725e-04  2.3643136e-04  5.1033497e-03  9.0092728e-03\n",
      " -9.3029495e-03 -7.1168090e-03  6.4588725e-03  8.9729885e-03\n",
      " -5.0154282e-03 -3.7633716e-03  7.3805046e-03 -1.5334714e-03\n",
      " -4.5366134e-03  6.5540518e-03 -4.8601604e-03 -1.8160177e-03\n",
      "  2.8765798e-03  9.9187379e-04 -8.2852151e-03 -9.4488179e-03\n",
      "  7.3117660e-03  5.0702621e-03  6.7576934e-03  7.6286553e-04\n",
      "  6.3508903e-03 -3.4053659e-03 -9.4640139e-04  5.7685734e-03\n",
      " -7.5216377e-03 -3.9361035e-03 -7.5115822e-03 -9.3004224e-04\n",
      "  9.5381187e-03 -7.3191668e-03 -2.3337686e-03 -1.9377411e-03\n",
      "  8.0774371e-03 -5.9308959e-03  4.5162440e-05 -4.7537340e-03\n",
      " -9.6035507e-03  5.0072931e-03 -8.7595852e-03 -4.3918253e-03\n",
      " -3.5099984e-05 -2.9618145e-04 -7.6612402e-03  9.6147433e-03\n",
      "  4.9820580e-03  9.2331432e-03 -8.1579173e-03  4.4957981e-03\n",
      " -4.1370760e-03  8.2453608e-04  8.4986202e-03 -4.4621765e-03\n",
      "  4.5175003e-03 -6.7869602e-03 -3.5484887e-03  9.3985079e-03\n",
      " -1.5776526e-03  3.2137157e-04 -4.1406299e-03 -7.6826881e-03\n",
      " -1.5080082e-03  2.4697948e-03 -8.8802696e-04  5.5336617e-03\n",
      " -2.7429771e-03  2.2600652e-03  5.4557943e-03  8.3459532e-03\n",
      " -1.4537406e-03 -9.2081428e-03  4.3705525e-03  5.7178497e-04\n",
      "  7.4419081e-03 -8.1328274e-04 -2.6384138e-03 -8.7530091e-03\n",
      " -8.5655687e-04  2.8265631e-03  5.4014288e-03  7.0526563e-03\n",
      " -5.7031214e-03  1.8588197e-03  6.0888636e-03 -4.7980510e-03\n",
      " -3.1072604e-03  6.7976294e-03  1.6314756e-03  1.8991709e-04\n",
      "  3.4736372e-03  2.1777749e-04  9.6188262e-03  5.0606038e-03\n",
      " -8.9173904e-03 -7.0415605e-03  9.0145587e-04  6.3925339e-03]\n"
     ]
    }
   ],
   "source": [
    "print(embedding_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93c952dd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
